{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf4b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (4940, 34), Test set shape: (380, 34)\n",
      "Class distribution â€” Train: [2237 2703], Test: [175 205]\n",
      "Starting cross-validation to generate stacking features...\n",
      "Processing fold 1/5...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2187\n",
      "[LightGBM] [Info] Number of data points in the train set: 3952, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.800421\n",
      "[LightGBM] [Info] Start training from score -0.596274\n",
      "Processing fold 2/5...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2187\n",
      "[LightGBM] [Info] Number of data points in the train set: 3952, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.795924\n",
      "[LightGBM] [Info] Start training from score -0.599956\n",
      "Processing fold 3/5...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2183\n",
      "[LightGBM] [Info] Number of data points in the train set: 3952, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.784215\n",
      "[LightGBM] [Info] Start training from score -0.609685\n",
      "Processing fold 4/5...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2186\n",
      "[LightGBM] [Info] Number of data points in the train set: 3952, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.786435\n",
      "[LightGBM] [Info] Start training from score -0.607824\n",
      "Processing fold 5/5...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2185\n",
      "[LightGBM] [Info] Number of data points in the train set: 3952, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.794243\n",
      "[LightGBM] [Info] Start training from score -0.601340\n",
      "Reshaped stacking features â€” Train: (4940, 3, 2), Test: (380, 3, 2)\n",
      "\n",
      "Base learner CV performance:\n",
      "XGBoost CV Accuracy: 0.6931\n",
      "LightGBM CV Accuracy: 0.6915\n",
      "RandomForest CV Accuracy: 0.7047\n",
      "\n",
      "Training LSTM stacking meta-learner...\n",
      "Epoch 1/50, Loss: 0.5958, Val Accuracy: 0.6579\n",
      "Epoch 2/50, Loss: 0.5906, Val Accuracy: 0.6609\n",
      "Epoch 3/50, Loss: 0.5909, Val Accuracy: 0.6660\n",
      "Epoch 4/50, Loss: 0.5897, Val Accuracy: 0.6660\n",
      "Epoch 5/50, Loss: 0.5894, Val Accuracy: 0.6670\n",
      "Epoch 6/50, Loss: 0.5884, Val Accuracy: 0.6660\n",
      "Epoch 7/50, Loss: 0.5892, Val Accuracy: 0.6650\n",
      "Epoch 8/50, Loss: 0.5890, Val Accuracy: 0.6650\n",
      "Epoch 9/50, Loss: 0.5911, Val Accuracy: 0.6670\n",
      "Epoch 10/50, Loss: 0.5860, Val Accuracy: 0.6660\n",
      "Epoch 11/50, Loss: 0.5875, Val Accuracy: 0.6640\n",
      "Epoch 12/50, Loss: 0.5902, Val Accuracy: 0.6660\n",
      "Epoch 13/50, Loss: 0.5873, Val Accuracy: 0.6690\n",
      "Epoch 14/50, Loss: 0.5901, Val Accuracy: 0.6680\n",
      "Epoch 15/50, Loss: 0.5890, Val Accuracy: 0.6680\n",
      "Epoch 16/50, Loss: 0.5880, Val Accuracy: 0.6690\n",
      "Epoch 17/50, Loss: 0.5876, Val Accuracy: 0.6660\n",
      "Epoch 18/50, Loss: 0.5848, Val Accuracy: 0.6650\n",
      "Epoch 19/50, Loss: 0.5876, Val Accuracy: 0.6650\n",
      "Epoch 20/50, Loss: 0.5860, Val Accuracy: 0.6650\n",
      "Epoch 21/50, Loss: 0.5871, Val Accuracy: 0.6670\n",
      "Epoch 22/50, Loss: 0.5876, Val Accuracy: 0.6690\n",
      "Epoch 23/50, Loss: 0.5876, Val Accuracy: 0.6650\n",
      "Early stopping at epoch 23\n",
      "\n",
      "=== LSTM Stacking Ensemble Performance ===\n",
      "âœ… Accuracy: 0.6789\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.60      0.63       175\n",
      "           1       0.69      0.75      0.71       205\n",
      "\n",
      "    accuracy                           0.68       380\n",
      "   macro avg       0.68      0.67      0.67       380\n",
      "weighted avg       0.68      0.68      0.68       380\n",
      "\n",
      "\n",
      "=== Training standalone LSTM model ===\n",
      "Epoch 1/50, Loss: 0.6256, Val Acc: 0.6559\n",
      "Epoch 2/50, Loss: 0.6123, Val Acc: 0.6802\n",
      "Epoch 3/50, Loss: 0.6091, Val Acc: 0.6872\n",
      "Epoch 4/50, Loss: 0.6080, Val Acc: 0.6812\n",
      "Epoch 5/50, Loss: 0.6040, Val Acc: 0.6893\n",
      "Epoch 6/50, Loss: 0.6004, Val Acc: 0.6872\n",
      "Epoch 7/50, Loss: 0.6004, Val Acc: 0.6984\n",
      "Epoch 8/50, Loss: 0.5973, Val Acc: 0.7055\n",
      "Epoch 9/50, Loss: 0.5949, Val Acc: 0.7065\n",
      "Epoch 10/50, Loss: 0.5905, Val Acc: 0.7115\n",
      "Epoch 11/50, Loss: 0.5896, Val Acc: 0.7055\n",
      "Epoch 12/50, Loss: 0.5898, Val Acc: 0.7014\n",
      "Epoch 13/50, Loss: 0.5857, Val Acc: 0.7368\n",
      "Epoch 14/50, Loss: 0.5808, Val Acc: 0.7176\n",
      "Epoch 15/50, Loss: 0.5824, Val Acc: 0.7085\n",
      "Epoch 16/50, Loss: 0.5799, Val Acc: 0.7115\n",
      "Epoch 17/50, Loss: 0.5743, Val Acc: 0.7470\n",
      "Epoch 18/50, Loss: 0.5707, Val Acc: 0.7298\n",
      "Epoch 19/50, Loss: 0.5720, Val Acc: 0.7520\n",
      "Epoch 20/50, Loss: 0.5679, Val Acc: 0.7389\n",
      "Epoch 21/50, Loss: 0.5644, Val Acc: 0.7581\n",
      "Epoch 22/50, Loss: 0.5569, Val Acc: 0.7500\n",
      "Epoch 23/50, Loss: 0.5569, Val Acc: 0.7581\n",
      "Epoch 24/50, Loss: 0.5567, Val Acc: 0.7257\n",
      "Epoch 25/50, Loss: 0.5567, Val Acc: 0.7611\n",
      "Epoch 26/50, Loss: 0.5526, Val Acc: 0.7692\n",
      "Epoch 27/50, Loss: 0.5523, Val Acc: 0.7692\n",
      "Epoch 28/50, Loss: 0.5464, Val Acc: 0.7794\n",
      "Epoch 29/50, Loss: 0.5418, Val Acc: 0.7773\n",
      "Epoch 30/50, Loss: 0.5393, Val Acc: 0.7692\n",
      "Epoch 31/50, Loss: 0.5357, Val Acc: 0.7844\n",
      "Epoch 32/50, Loss: 0.5289, Val Acc: 0.7804\n",
      "Epoch 33/50, Loss: 0.5295, Val Acc: 0.7854\n",
      "Epoch 34/50, Loss: 0.5264, Val Acc: 0.8006\n",
      "Epoch 35/50, Loss: 0.5264, Val Acc: 0.8077\n",
      "Epoch 36/50, Loss: 0.5241, Val Acc: 0.7976\n",
      "Epoch 37/50, Loss: 0.5202, Val Acc: 0.8138\n",
      "Epoch 38/50, Loss: 0.5116, Val Acc: 0.8087\n",
      "Epoch 39/50, Loss: 0.5098, Val Acc: 0.8097\n",
      "Epoch 40/50, Loss: 0.5065, Val Acc: 0.8087\n",
      "Epoch 41/50, Loss: 0.5102, Val Acc: 0.8097\n",
      "Epoch 42/50, Loss: 0.5027, Val Acc: 0.8269\n",
      "Epoch 43/50, Loss: 0.5016, Val Acc: 0.8239\n",
      "Epoch 44/50, Loss: 0.5036, Val Acc: 0.8259\n",
      "Epoch 45/50, Loss: 0.4971, Val Acc: 0.8259\n",
      "Epoch 46/50, Loss: 0.4977, Val Acc: 0.8279\n",
      "Epoch 47/50, Loss: 0.4965, Val Acc: 0.8219\n",
      "Epoch 48/50, Loss: 0.4914, Val Acc: 0.8289\n",
      "Epoch 49/50, Loss: 0.4928, Val Acc: 0.8401\n",
      "Epoch 50/50, Loss: 0.4845, Val Acc: 0.8421\n",
      "\n",
      "=== Standalone LSTM Performance ===\n",
      "âœ… Accuracy: 0.6421\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.62      0.61       175\n",
      "           1       0.67      0.66      0.67       205\n",
      "\n",
      "    accuracy                           0.64       380\n",
      "   macro avg       0.64      0.64      0.64       380\n",
      "weighted avg       0.64      0.64      0.64       380\n",
      "\n",
      "\n",
      "=== Majority Voting Performance ===\n",
      "âœ… Accuracy: 0.6816\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.66      0.66       175\n",
      "           1       0.71      0.70      0.70       205\n",
      "\n",
      "    accuracy                           0.68       380\n",
      "   macro avg       0.68      0.68      0.68       380\n",
      "weighted avg       0.68      0.68      0.68       380\n",
      "\n",
      "\n",
      "=== Baseline Model Reports ===\n",
      "\n",
      "---- XGBoost ----\n",
      "Accuracy: 0.6579\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63       175\n",
      "           1       0.68      0.69      0.68       205\n",
      "\n",
      "    accuracy                           0.66       380\n",
      "   macro avg       0.66      0.66      0.66       380\n",
      "weighted avg       0.66      0.66      0.66       380\n",
      "\n",
      "\n",
      "---- LightGBM ----\n",
      "Accuracy: 0.6763\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.65       175\n",
      "           1       0.70      0.71      0.70       205\n",
      "\n",
      "    accuracy                           0.68       380\n",
      "   macro avg       0.67      0.67      0.67       380\n",
      "weighted avg       0.68      0.68      0.68       380\n",
      "\n",
      "\n",
      "---- RandomForest ----\n",
      "Accuracy: 0.6816\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.65       175\n",
      "           1       0.70      0.71      0.71       205\n",
      "\n",
      "    accuracy                           0.68       380\n",
      "   macro avg       0.68      0.68      0.68       380\n",
      "weighted avg       0.68      0.68      0.68       380\n",
      "\n",
      "\n",
      "=== Test Set Performance Comparison ===\n",
      "XGBoost: 0.6579\n",
      "LightGBM: 0.6763\n",
      "RandomForest: 0.6816\n",
      "Voting: 0.6816\n",
      "LSTM Stack: 0.6789\n",
      "Single LSTM: 0.6421\n",
      "\n",
      "All training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === Load data ===\n",
    "dataTrain = pd.read_csv(\"allAtt_onehot_large_train_new8.csv\")\n",
    "dataTest = pd.read_csv(\"allAtt_onehot_large_test_new8.csv\")\n",
    "\n",
    "x_train, y_train = dataTrain.iloc[:, 4:38].values, dataTrain.iloc[:, 38:].values\n",
    "x_test, y_test = dataTest.iloc[:, 4:38].values, dataTest.iloc[:, 38:].values\n",
    "\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(f\"Train set shape: {x_train.shape}, Test set shape: {x_test.shape}\")\n",
    "print(f\"Class distribution â€” Train: {np.bincount(y_train_int)}, Test: {np.bincount(y_test_int)}\")\n",
    "\n",
    "# === Generate stacking features via cross-validation ===\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "n_classes = 2\n",
    "\n",
    "# Storage for train-set predictions\n",
    "xgb_train_preds = np.zeros((n_train, n_classes))\n",
    "lgb_train_preds = np.zeros((n_train, n_classes))\n",
    "rf_train_preds  = np.zeros((n_train, n_classes))\n",
    "\n",
    "# Storage for test-set predictions (we'll average across folds)\n",
    "xgb_test_preds = np.zeros((n_test, n_classes))\n",
    "lgb_test_preds = np.zeros((n_test, n_classes))\n",
    "rf_test_preds  = np.zeros((n_test, n_classes))\n",
    "\n",
    "print(\"Starting cross-validation to generate stacking features...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"Processing fold {fold+1}/{n_folds}...\")\n",
    "    \n",
    "    X_fold_train, X_fold_val = x_train[train_idx], x_train[val_idx]\n",
    "    y_fold_train = y_train_int[train_idx]\n",
    "    \n",
    "    # Initialize base models\n",
    "    fold_xgb = XGBClassifier(objective=\"multi:softprob\", num_class=2,\n",
    "                             eval_metric=\"mlogloss\", use_label_encoder=False,\n",
    "                             random_state=42)\n",
    "    fold_lgb = LGBMClassifier(objective='multiclass', num_class=2,\n",
    "                              random_state=42)\n",
    "    fold_rf  = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Fit models\n",
    "    fold_xgb.fit(X_fold_train, y_fold_train)\n",
    "    fold_lgb.fit(X_fold_train, y_fold_train)\n",
    "    fold_rf.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Predict on validation fold\n",
    "    xgb_train_preds[val_idx] = fold_xgb.predict_proba(X_fold_val)\n",
    "    lgb_train_preds[val_idx] = fold_lgb.predict_proba(X_fold_val)\n",
    "    rf_train_preds[val_idx]  = fold_rf.predict_proba(X_fold_val)\n",
    "    \n",
    "    # Predict on test set and accumulate\n",
    "    xgb_test_preds += fold_xgb.predict_proba(x_test) / n_folds\n",
    "    lgb_test_preds += fold_lgb.predict_proba(x_test) / n_folds\n",
    "    rf_test_preds  += fold_rf.predict_proba(x_test) / n_folds\n",
    "\n",
    "# === Prepare stacking features for LSTM ===\n",
    "n_models = 3  # XGBoost, LightGBM, RandomForest\n",
    "\n",
    "train_probs_reshaped = np.stack([xgb_train_preds, lgb_train_preds, rf_train_preds], axis=1)\n",
    "test_probs_reshaped  = np.stack([xgb_test_preds,  lgb_test_preds,  rf_test_preds ], axis=1)\n",
    "\n",
    "print(f\"Reshaped stacking features â€” Train: {train_probs_reshaped.shape}, Test: {test_probs_reshaped.shape}\")\n",
    "\n",
    "# === Evaluate base learners on CV predictions ===\n",
    "base_models = ['XGBoost', 'LightGBM', 'RandomForest']\n",
    "base_preds = [\n",
    "    np.argmax(xgb_train_preds, axis=1),\n",
    "    np.argmax(lgb_train_preds, axis=1),\n",
    "    np.argmax(rf_train_preds,  axis=1)\n",
    "]\n",
    "\n",
    "print(\"\\nBase learner CV performance:\")\n",
    "for name, preds in zip(base_models, base_preds):\n",
    "    acc = accuracy_score(y_train_int, preds)\n",
    "    print(f\"{name} CV Accuracy: {acc:.4f}\")\n",
    "\n",
    "# === Define LSTM stacking meta-learner ===\n",
    "class LSTMStack(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.1, bidirectional=True, output_dim=2):\n",
    "        super(LSTMStack, self).__init__()\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers>1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size*self.directions, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size*self.directions, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, input_size]\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden*dirs]\n",
    "        attn_scores = self.attention(lstm_out)  # [batch, seq_len, 1]\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)  # [batch, hidden*dirs]\n",
    "        out = self.fc(context)\n",
    "        return self.softmax(out)\n",
    "\n",
    "# Convert to tensors and DataLoader\n",
    "train_tensor = torch.FloatTensor(train_probs_reshaped)\n",
    "test_tensor  = torch.FloatTensor(test_probs_reshaped)\n",
    "y_train_t    = torch.LongTensor(y_train_int)\n",
    "y_test_t     = torch.LongTensor(y_test_int)\n",
    "\n",
    "train_ds = TensorDataset(train_tensor, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize LSTMStack\n",
    "input_size = n_classes\n",
    "hidden_size = 64\n",
    "model = LSTMStack(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    bidirectional=True,\n",
    "    output_dim=2\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# === Train LSTM stacking meta-learner ===\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "best_model_path = 'best_lstm_stack_model.pt'\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"\\nTraining LSTM stacking meta-learner...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Validation on last 20% of training data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_size = int(0.2 * len(train_tensor))\n",
    "        val_inputs = train_tensor[-val_size:].to(device)\n",
    "        val_labels = y_train_t[-val_size:].to(device)\n",
    "        \n",
    "        val_outputs = model(val_inputs)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        val_acc = accuracy_score(val_labels.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# === Plot training history ===\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# === Evaluate LSTM stacking meta-learner ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_tensor.to(device))\n",
    "    _, preds_stack = torch.max(outputs, 1)\n",
    "    preds_stack = preds_stack.cpu().numpy()\n",
    "\n",
    "stack_acc = accuracy_score(y_test_int, preds_stack)\n",
    "stack_report = classification_report(y_test_int, preds_stack)\n",
    "stack_cm = confusion_matrix(y_test_int, preds_stack)\n",
    "\n",
    "print(\"\\n=== LSTM Stacking Ensemble Performance ===\")\n",
    "print(f\"âœ… Accuracy: {stack_acc:.4f}\")\n",
    "print(\"ðŸ“Š Classification Report:\")\n",
    "print(stack_report)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(stack_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('LSTM Stacking Confusion Matrix')\n",
    "plt.savefig('lstm_stack_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# ============ Single LSTM Model ============\n",
    "print(\"\\n=== Training standalone LSTM model ===\")\n",
    "\n",
    "feature_dim = x_train.shape[1]\n",
    "time_steps = 1\n",
    "\n",
    "x_train_lstm = x_train.reshape(n_train, time_steps, feature_dim)\n",
    "x_test_lstm  = x_test.reshape(n_test, time_steps, feature_dim)\n",
    "\n",
    "x_train_tensor = torch.FloatTensor(x_train_lstm)\n",
    "x_test_tensor  = torch.FloatTensor(x_test_lstm)\n",
    "\n",
    "lstm_ds = TensorDataset(x_train_tensor, y_train_t)\n",
    "lstm_loader = DataLoader(lstm_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.1, bidirectional=True, output_dim=2):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers>1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size*self.directions, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(last_out)\n",
    "        return self.softmax(out)\n",
    "\n",
    "single_lstm = SimpleLSTM(\n",
    "    input_size=feature_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    bidirectional=True,\n",
    "    output_dim=2\n",
    ").to(device)\n",
    "\n",
    "lstm_criterion = nn.CrossEntropyLoss()\n",
    "lstm_optimizer = optim.Adam(single_lstm.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "lstm_scheduler = optim.lr_scheduler.ReduceLROnPlateau(lstm_optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "lstm_epochs = 50\n",
    "best_val_loss2 = float('inf')\n",
    "patience2 = 10\n",
    "counter2 = 0\n",
    "best_single_path = 'best_single_lstm_model.pt'\n",
    "\n",
    "train_losses2 = []\n",
    "val_acc2 = []\n",
    "\n",
    "for epoch in range(lstm_epochs):\n",
    "    single_lstm.train()\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in lstm_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = single_lstm(inputs)\n",
    "        loss = lstm_criterion(outputs, labels)\n",
    "        \n",
    "        lstm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(single_lstm.parameters(), max_norm=1.0)\n",
    "        lstm_optimizer.step()\n",
    "        \n",
    "        total += loss.item()\n",
    "    \n",
    "    avg = total / len(lstm_loader)\n",
    "    train_losses2.append(avg)\n",
    "    \n",
    "    single_lstm.eval()\n",
    "    with torch.no_grad():\n",
    "        val_size = int(0.2 * len(x_train_tensor))\n",
    "        v_inputs = x_train_tensor[-val_size:].to(device)\n",
    "        v_labels = y_train_t[-val_size:].to(device)\n",
    "        \n",
    "        v_out = single_lstm(v_inputs)\n",
    "        v_loss = lstm_criterion(v_out, v_labels)\n",
    "        _, v_preds = torch.max(v_out, 1)\n",
    "        v_acc = accuracy_score(v_labels.cpu().numpy(), v_preds.cpu().numpy())\n",
    "        val_acc2.append(v_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{lstm_epochs}, Loss: {avg:.4f}, Val Acc: {v_acc:.4f}\")\n",
    "        \n",
    "        if v_loss < best_val_loss2:\n",
    "            best_val_loss2 = v_loss\n",
    "            counter2 = 0\n",
    "            torch.save(single_lstm.state_dict(), best_single_path)\n",
    "        else:\n",
    "            counter2 += 1\n",
    "            if counter2 >= patience2:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        lstm_scheduler.step(v_loss)\n",
    "\n",
    "single_lstm.load_state_dict(torch.load(best_single_path))\n",
    "\n",
    "# Plot standalone LSTM history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses2)\n",
    "plt.title('Standalone LSTM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_acc2)\n",
    "plt.title('Standalone LSTM Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('single_lstm_training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Evaluate standalone LSTM\n",
    "single_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    outs = single_lstm(x_test_tensor.to(device))\n",
    "    _, preds_single = torch.max(outs, 1)\n",
    "    preds_single = preds_single.cpu().numpy()\n",
    "\n",
    "single_acc = accuracy_score(y_test_int, preds_single)\n",
    "single_report = classification_report(y_test_int, preds_single)\n",
    "single_cm = confusion_matrix(y_test_int, preds_single)\n",
    "\n",
    "print(\"\\n=== Standalone LSTM Performance ===\")\n",
    "print(f\"âœ… Accuracy: {single_acc:.4f}\")\n",
    "print(\"ðŸ“Š Classification Report:\")\n",
    "print(single_report)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(single_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Standalone LSTM Confusion Matrix')\n",
    "plt.savefig('single_lstm_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# === Majority Voting Ensemble ===\n",
    "test_base_preds = [\n",
    "    np.argmax(xgb_test_preds, axis=1),\n",
    "    np.argmax(lgb_test_preds, axis=1),\n",
    "    np.argmax(rf_test_preds,  axis=1)\n",
    "]\n",
    "\n",
    "votes = np.array(test_base_preds).T\n",
    "vote_result = np.array([np.bincount(row).argmax() for row in votes])\n",
    "\n",
    "vote_acc = accuracy_score(y_test_int, vote_result)\n",
    "vote_report = classification_report(y_test_int, vote_result)\n",
    "\n",
    "print(\"\\n=== Majority Voting Performance ===\")\n",
    "print(f\"âœ… Accuracy: {vote_acc:.4f}\")\n",
    "print(\"ðŸ“Š Classification Report:\")\n",
    "print(vote_report)\n",
    "\n",
    "# === Baseline model reports ===\n",
    "print(\"\\n=== Baseline Model Reports ===\")\n",
    "for name, preds in zip(base_models, test_base_preds):\n",
    "    print(f\"\\n---- {name} ----\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test_int, preds):.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test_int, preds))\n",
    "\n",
    "# === Compare all models ===\n",
    "base_accs = [accuracy_score(y_test_int, preds) for preds in test_base_preds]\n",
    "model_names = base_models + ['Voting', 'LSTM Stack', 'Single LSTM']\n",
    "model_accs = base_accs + [vote_acc, stack_acc, single_acc]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(model_names, model_accs)\n",
    "\n",
    "for bar, acc in zip(bars, model_accs):\n",
    "    plt.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.01,\n",
    "             f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.ylim(0, max(model_accs)+0.1)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Set Performance Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_models_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== Test Set Performance Comparison ===\")\n",
    "for name, acc in zip(model_names, model_accs):\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nAll training and evaluation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
