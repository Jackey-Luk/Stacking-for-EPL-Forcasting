{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10227329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === 加载数据 ===\n",
    "dataTrain = pd.read_csv(\"allAtt_onehot_large_train_new8.csv\")\n",
    "dataTest = pd.read_csv(\"allAtt_onehot_large_test_new8.csv\")\n",
    "\n",
    "x_train, y_train = dataTrain.iloc[:, 4:38].values, dataTrain.iloc[:, 38:].values\n",
    "x_test, y_test = dataTest.iloc[:, 4:38].values, dataTest.iloc[:, 38:].values\n",
    "\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(f\"训练集形状: {x_train.shape}, 测试集形状: {x_test.shape}\")\n",
    "print(f\"类别分布 - 训练集: {np.bincount(y_train_int)}, 测试集: {np.bincount(y_test_int)}\")\n",
    "\n",
    "# === 使用交叉验证生成堆叠特征 ===\n",
    "# 初始化K折交叉验证\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 初始化存储训练集预测的数组\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "n_classes = 2\n",
    "\n",
    "# 用于存储每个折叠的训练集预测\n",
    "xgb_train_preds = np.zeros((n_train, n_classes))\n",
    "lgb_train_preds = np.zeros((n_train, n_classes))\n",
    "rf_train_preds = np.zeros((n_train, n_classes))\n",
    "\n",
    "# 用于存储测试集预测（我们将平均每个折叠的预测）\n",
    "xgb_test_preds = np.zeros((n_test, n_classes))\n",
    "lgb_test_preds = np.zeros((n_test, n_classes))\n",
    "rf_test_preds = np.zeros((n_test, n_classes))\n",
    "\n",
    "print(\"开始进行交叉验证生成堆叠特征...\")\n",
    "\n",
    "# 执行交叉验证\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"处理第 {fold+1}/{n_folds} 折...\")\n",
    "    \n",
    "    # 分割数据\n",
    "    X_fold_train, X_fold_val = x_train[train_idx], x_train[val_idx]\n",
    "    y_fold_train = y_train_int[train_idx]\n",
    "    \n",
    "    # 初始化和训练模型\n",
    "    fold_xgb = XGBClassifier(objective=\"multi:softprob\", num_class=2, eval_metric=\"mlogloss\", \n",
    "                             use_label_encoder=False, random_state=42)\n",
    "    fold_lgb = LGBMClassifier(objective='multiclass', num_class=2, random_state=42)\n",
    "    fold_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # 训练模型\n",
    "    fold_xgb.fit(X_fold_train, y_fold_train)\n",
    "    fold_lgb.fit(X_fold_train, y_fold_train)\n",
    "    fold_rf.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # 对验证集进行预测（当前折叠中的验证数据）\n",
    "    xgb_train_preds[val_idx] = fold_xgb.predict_proba(X_fold_val)\n",
    "    lgb_train_preds[val_idx] = fold_lgb.predict_proba(X_fold_val)\n",
    "    rf_train_preds[val_idx] = fold_rf.predict_proba(X_fold_val)\n",
    "    \n",
    "    # 对测试集进行预测并累积（稍后我们将取平均值）\n",
    "    xgb_test_preds += fold_xgb.predict_proba(x_test) / n_folds\n",
    "    lgb_test_preds += fold_lgb.predict_proba(x_test) / n_folds\n",
    "    rf_test_preds += fold_rf.predict_proba(x_test) / n_folds\n",
    "\n",
    "# === 为LSTM准备堆叠特征 ===\n",
    "# 基础模型数量和每个模型输出的类别数\n",
    "n_models = 3  # XGBoost, LightGBM, RandomForest\n",
    "n_classes = 2  # 二分类问题\n",
    "\n",
    "# 将预测概率重塑为适合LSTM的格式 [样本数, 时间步(模型数), 特征(类别数)]\n",
    "train_probs_reshaped = np.zeros((n_train, n_models, n_classes))\n",
    "train_probs_reshaped[:, 0, :] = xgb_train_preds\n",
    "train_probs_reshaped[:, 1, :] = lgb_train_preds\n",
    "train_probs_reshaped[:, 2, :] = rf_train_preds\n",
    "\n",
    "test_probs_reshaped = np.zeros((n_test, n_models, n_classes))\n",
    "test_probs_reshaped[:, 0, :] = xgb_test_preds\n",
    "test_probs_reshaped[:, 1, :] = lgb_test_preds\n",
    "test_probs_reshaped[:, 2, :] = rf_test_preds\n",
    "\n",
    "print(f\"重塑后的堆叠特征形状 - 训练集: {train_probs_reshaped.shape}, 测试集: {test_probs_reshaped.shape}\")\n",
    "\n",
    "# === 基础学习器性能评估 ===\n",
    "base_models = ['XGBoost', 'LightGBM', 'RandomForest']\n",
    "base_preds = [\n",
    "    np.argmax(xgb_train_preds, axis=1),\n",
    "    np.argmax(lgb_train_preds, axis=1),\n",
    "    np.argmax(rf_train_preds, axis=1)\n",
    "]\n",
    "\n",
    "print(\"\\n基础学习器在交叉验证上的性能:\")\n",
    "for name, preds in zip(base_models, base_preds):\n",
    "    acc = accuracy_score(y_train_int, preds)\n",
    "    print(f\"{name} CV 准确率: {acc:.4f}\")\n",
    "\n",
    "# === 构建 LSTM 元模型 ===\n",
    "class LSTMStack(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.3, bidirectional=True, output_dim=2):\n",
    "        super(LSTMStack, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # 注意力机制\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.directions, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.directions, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x形状: [batch_size, seq_len, input_size]\n",
    "        \n",
    "        # LSTM处理\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_size*directions]\n",
    "        \n",
    "        # 注意力计算\n",
    "        attention_scores = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # 加权平均\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # [batch_size, hidden_size*directions]\n",
    "        \n",
    "        # 输出层\n",
    "        out = self.fc(context)\n",
    "        return self.softmax(out)\n",
    "\n",
    "# 转换为PyTorch tensors\n",
    "train_probs_tensor = torch.FloatTensor(train_probs_reshaped)\n",
    "test_probs_tensor = torch.FloatTensor(test_probs_reshaped)\n",
    "y_train_tensor = torch.LongTensor(y_train_int)\n",
    "y_test_tensor = torch.LongTensor(y_test_int)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(train_probs_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 初始化LSTM模型\n",
    "input_size = n_classes  # 每个时间步(模型)输出的类别数\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "model = LSTMStack(\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    num_layers=num_layers, \n",
    "    dropout=0.3, \n",
    "    bidirectional=True,\n",
    "    output_dim=2\n",
    ")\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# === 训练LSTM元模型 ===\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 添加早停\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "best_model_path = 'best_lstm_stack_model.pt'\n",
    "\n",
    "# 跟踪训练历史\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"\\n开始训练LSTM元学习器...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # 验证\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 使用一部分训练数据作为验证集\n",
    "        val_size = int(0.2 * len(train_probs_tensor))\n",
    "        val_inputs = train_probs_tensor[-val_size:].to(device)\n",
    "        val_labels = y_train_tensor[-val_size:].to(device)\n",
    "        \n",
    "        val_outputs = model(val_inputs)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "        \n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        val_accuracy = accuracy_score(val_labels.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # 早停逻辑\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        # 学习率调整\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# === 绘制训练历史 ===\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('训练损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies)\n",
    "plt.title('验证准确率')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# === LSTM元模型评估 ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_probs_tensor = test_probs_tensor.to(device)\n",
    "    outputs = model(test_probs_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    predicted = predicted.cpu().numpy()\n",
    "    \n",
    "# 计算性能指标\n",
    "lstm_acc = accuracy_score(y_test_int, predicted)\n",
    "lstm_report = classification_report(y_test_int, predicted)\n",
    "lstm_cm = confusion_matrix(y_test_int, predicted)\n",
    "\n",
    "print(\"\\n=== LSTM元模型性能 ===\")\n",
    "print(f\"✅ LSTM堆叠集成准确率: {lstm_acc:.4f}\")\n",
    "print(\"📊 分类报告:\")\n",
    "print(lstm_report)\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lstm_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "plt.xlabel('预测标签')\n",
    "plt.ylabel('真实标签')\n",
    "plt.title('LSTM元模型混淆矩阵')\n",
    "plt.savefig('lstm_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# === 与投票系统比较 ===\n",
    "# 对测试集进行多数投票\n",
    "test_base_preds = [\n",
    "    np.argmax(xgb_test_preds, axis=1),\n",
    "    np.argmax(lgb_test_preds, axis=1),\n",
    "    np.argmax(rf_test_preds, axis=1)\n",
    "]\n",
    "\n",
    "# 转换为数组以便投票\n",
    "test_votes = np.array(test_base_preds).T\n",
    "vote_result = np.array([np.bincount(row).argmax() for row in test_votes])\n",
    "\n",
    "vote_acc = accuracy_score(y_test_int, vote_result)\n",
    "vote_report = classification_report(y_test_int, vote_result)\n",
    "\n",
    "print(\"\\n=== 多数投票系统性能 ===\")\n",
    "print(f\"✅ 投票系统准确率: {vote_acc:.4f}\")\n",
    "print(\"📊 分类报告:\")\n",
    "print(vote_report)\n",
    "\n",
    "# 在这里加上基线模型报告\n",
    "print(\"\\n=== 基线模型分类报告 ===\")\n",
    "for name, preds in zip(base_models, test_base_preds):\n",
    "    print(f\"\\n---- {name} ----\")\n",
    "    print(f\"准确率: {accuracy_score(y_test_int, preds):.4f}\")\n",
    "    print(\"分类报告:\")\n",
    "    print(classification_report(y_test_int, preds))\n",
    "\n",
    "# === 比较所有模型性能 ===\n",
    "base_accs = [accuracy_score(y_test_int, preds) for preds in test_base_preds]\n",
    "\n",
    "model_names = base_models + ['投票系统', 'LSTM元模型']\n",
    "model_accs = base_accs + [vote_acc, lstm_acc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, model_accs, \n",
    "               color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "\n",
    "# 在柱状图上添加准确率数值\n",
    "for bar, acc_val in zip(bars, model_accs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc_val:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.ylim(0, max(model_accs) + 0.1)\n",
    "plt.xlabel('模型')\n",
    "plt.ylabel('准确率')\n",
    "plt.title('各个模型在测试集上的性能比较')\n",
    "plt.savefig('lstm_model_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== 各个模型在测试集上的性能比较 ===\")\n",
    "for name, acc_val in zip(model_names, model_accs):\n",
    "    print(f\"{name}: {acc_val:.4f}\")\n",
    "\n",
    "# 可视化注意力权重分析\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 获取一批测试数据\n",
    "    sample_inputs = test_probs_tensor[:20].to(device)\n",
    "    \n",
    "    # 前向传播并获取注意力权重\n",
    "    lstm_out, _ = model.lstm(sample_inputs)\n",
    "    attention_scores = model.attention(lstm_out)\n",
    "    attention_weights = torch.softmax(attention_scores, dim=1).cpu().numpy()\n",
    "    \n",
    "    # 计算每个模型的平均注意力权重\n",
    "    avg_attention = attention_weights.mean(axis=0).flatten()\n",
    "    \n",
    "    # 可视化\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(base_models, avg_attention, color='skyblue')\n",
    "    plt.xlabel('基础模型')\n",
    "    plt.ylabel('平均注意力权重')\n",
    "    plt.title('LSTM元模型对各基础模型的注意力分配')\n",
    "    plt.savefig('attention_weights.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n=== 注意力权重分析 ===\")\n",
    "    for name, weight in zip(base_models, avg_attention):\n",
    "        print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nLSTM堆叠集成训练完成！\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
