{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10227329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "dataTrain = pd.read_csv(\"allAtt_onehot_large_train_new8.csv\")\n",
    "dataTest = pd.read_csv(\"allAtt_onehot_large_test_new8.csv\")\n",
    "\n",
    "x_train, y_train = dataTrain.iloc[:, 4:38].values, dataTrain.iloc[:, 38:].values\n",
    "x_test, y_test = dataTest.iloc[:, 4:38].values, dataTest.iloc[:, 38:].values\n",
    "\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å½¢çŠ¶: {x_train.shape}, æµ‹è¯•é›†å½¢çŠ¶: {x_test.shape}\")\n",
    "print(f\"ç±»åˆ«åˆ†å¸ƒ - è®­ç»ƒé›†: {np.bincount(y_train_int)}, æµ‹è¯•é›†: {np.bincount(y_test_int)}\")\n",
    "\n",
    "# === ä½¿ç”¨äº¤å‰éªŒè¯ç”Ÿæˆå †å ç‰¹å¾ ===\n",
    "# åˆå§‹åŒ–KæŠ˜äº¤å‰éªŒè¯\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# åˆå§‹åŒ–å­˜å‚¨è®­ç»ƒé›†é¢„æµ‹çš„æ•°ç»„\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "n_classes = 2\n",
    "\n",
    "# ç”¨äºå­˜å‚¨æ¯ä¸ªæŠ˜å çš„è®­ç»ƒé›†é¢„æµ‹\n",
    "xgb_train_preds = np.zeros((n_train, n_classes))\n",
    "lgb_train_preds = np.zeros((n_train, n_classes))\n",
    "rf_train_preds = np.zeros((n_train, n_classes))\n",
    "\n",
    "# ç”¨äºå­˜å‚¨æµ‹è¯•é›†é¢„æµ‹ï¼ˆæˆ‘ä»¬å°†å¹³å‡æ¯ä¸ªæŠ˜å çš„é¢„æµ‹ï¼‰\n",
    "xgb_test_preds = np.zeros((n_test, n_classes))\n",
    "lgb_test_preds = np.zeros((n_test, n_classes))\n",
    "rf_test_preds = np.zeros((n_test, n_classes))\n",
    "\n",
    "print(\"å¼€å§‹è¿›è¡Œäº¤å‰éªŒè¯ç”Ÿæˆå †å ç‰¹å¾...\")\n",
    "\n",
    "# æ‰§è¡Œäº¤å‰éªŒè¯\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"å¤„ç†ç¬¬ {fold+1}/{n_folds} æŠ˜...\")\n",
    "    \n",
    "    # åˆ†å‰²æ•°æ®\n",
    "    X_fold_train, X_fold_val = x_train[train_idx], x_train[val_idx]\n",
    "    y_fold_train = y_train_int[train_idx]\n",
    "    \n",
    "    # åˆå§‹åŒ–å’Œè®­ç»ƒæ¨¡å‹\n",
    "    fold_xgb = XGBClassifier(objective=\"multi:softprob\", num_class=2, eval_metric=\"mlogloss\", \n",
    "                             use_label_encoder=False, random_state=42)\n",
    "    fold_lgb = LGBMClassifier(objective='multiclass', num_class=2, random_state=42)\n",
    "    fold_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    fold_xgb.fit(X_fold_train, y_fold_train)\n",
    "    fold_lgb.fit(X_fold_train, y_fold_train)\n",
    "    fold_rf.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # å¯¹éªŒè¯é›†è¿›è¡Œé¢„æµ‹ï¼ˆå½“å‰æŠ˜å ä¸­çš„éªŒè¯æ•°æ®ï¼‰\n",
    "    xgb_train_preds[val_idx] = fold_xgb.predict_proba(X_fold_val)\n",
    "    lgb_train_preds[val_idx] = fold_lgb.predict_proba(X_fold_val)\n",
    "    rf_train_preds[val_idx] = fold_rf.predict_proba(X_fold_val)\n",
    "    \n",
    "    # å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹å¹¶ç´¯ç§¯ï¼ˆç¨åæˆ‘ä»¬å°†å–å¹³å‡å€¼ï¼‰\n",
    "    xgb_test_preds += fold_xgb.predict_proba(x_test) / n_folds\n",
    "    lgb_test_preds += fold_lgb.predict_proba(x_test) / n_folds\n",
    "    rf_test_preds += fold_rf.predict_proba(x_test) / n_folds\n",
    "\n",
    "# === ä¸ºLSTMå‡†å¤‡å †å ç‰¹å¾ ===\n",
    "# åŸºç¡€æ¨¡å‹æ•°é‡å’Œæ¯ä¸ªæ¨¡å‹è¾“å‡ºçš„ç±»åˆ«æ•°\n",
    "n_models = 3  # XGBoost, LightGBM, RandomForest\n",
    "n_classes = 2  # äºŒåˆ†ç±»é—®é¢˜\n",
    "\n",
    "# å°†é¢„æµ‹æ¦‚ç‡é‡å¡‘ä¸ºé€‚åˆLSTMçš„æ ¼å¼ [æ ·æœ¬æ•°, æ—¶é—´æ­¥(æ¨¡å‹æ•°), ç‰¹å¾(ç±»åˆ«æ•°)]\n",
    "train_probs_reshaped = np.zeros((n_train, n_models, n_classes))\n",
    "train_probs_reshaped[:, 0, :] = xgb_train_preds\n",
    "train_probs_reshaped[:, 1, :] = lgb_train_preds\n",
    "train_probs_reshaped[:, 2, :] = rf_train_preds\n",
    "\n",
    "test_probs_reshaped = np.zeros((n_test, n_models, n_classes))\n",
    "test_probs_reshaped[:, 0, :] = xgb_test_preds\n",
    "test_probs_reshaped[:, 1, :] = lgb_test_preds\n",
    "test_probs_reshaped[:, 2, :] = rf_test_preds\n",
    "\n",
    "print(f\"é‡å¡‘åçš„å †å ç‰¹å¾å½¢çŠ¶ - è®­ç»ƒé›†: {train_probs_reshaped.shape}, æµ‹è¯•é›†: {test_probs_reshaped.shape}\")\n",
    "\n",
    "# === åŸºç¡€å­¦ä¹ å™¨æ€§èƒ½è¯„ä¼° ===\n",
    "base_models = ['XGBoost', 'LightGBM', 'RandomForest']\n",
    "base_preds = [\n",
    "    np.argmax(xgb_train_preds, axis=1),\n",
    "    np.argmax(lgb_train_preds, axis=1),\n",
    "    np.argmax(rf_train_preds, axis=1)\n",
    "]\n",
    "\n",
    "print(\"\\nåŸºç¡€å­¦ä¹ å™¨åœ¨äº¤å‰éªŒè¯ä¸Šçš„æ€§èƒ½:\")\n",
    "for name, preds in zip(base_models, base_preds):\n",
    "    acc = accuracy_score(y_train_int, preds)\n",
    "    print(f\"{name} CV å‡†ç¡®ç‡: {acc:.4f}\")\n",
    "\n",
    "# === æ„å»º LSTM å…ƒæ¨¡å‹ ===\n",
    "class LSTMStack(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.3, bidirectional=True, output_dim=2):\n",
    "        super(LSTMStack, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # LSTMå±‚\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # æ³¨æ„åŠ›æœºåˆ¶\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.directions, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.directions, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # xå½¢çŠ¶: [batch_size, seq_len, input_size]\n",
    "        \n",
    "        # LSTMå¤„ç†\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_size*directions]\n",
    "        \n",
    "        # æ³¨æ„åŠ›è®¡ç®—\n",
    "        attention_scores = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # åŠ æƒå¹³å‡\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # [batch_size, hidden_size*directions]\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        out = self.fc(context)\n",
    "        return self.softmax(out)\n",
    "\n",
    "# è½¬æ¢ä¸ºPyTorch tensors\n",
    "train_probs_tensor = torch.FloatTensor(train_probs_reshaped)\n",
    "test_probs_tensor = torch.FloatTensor(test_probs_reshaped)\n",
    "y_train_tensor = torch.LongTensor(y_train_int)\n",
    "y_test_tensor = torch.LongTensor(y_test_int)\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "train_dataset = TensorDataset(train_probs_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# åˆå§‹åŒ–LSTMæ¨¡å‹\n",
    "input_size = n_classes  # æ¯ä¸ªæ—¶é—´æ­¥(æ¨¡å‹)è¾“å‡ºçš„ç±»åˆ«æ•°\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "model = LSTMStack(\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    num_layers=num_layers, \n",
    "    dropout=0.3, \n",
    "    bidirectional=True,\n",
    "    output_dim=2\n",
    ")\n",
    "\n",
    "# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# === è®­ç»ƒLSTMå…ƒæ¨¡å‹ ===\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# æ·»åŠ æ—©åœ\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "best_model_path = 'best_lstm_stack_model.pt'\n",
    "\n",
    "# è·Ÿè¸ªè®­ç»ƒå†å²\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒLSTMå…ƒå­¦ä¹ å™¨...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # éªŒè¯\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # ä½¿ç”¨ä¸€éƒ¨åˆ†è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯é›†\n",
    "        val_size = int(0.2 * len(train_probs_tensor))\n",
    "        val_inputs = train_probs_tensor[-val_size:].to(device)\n",
    "        val_labels = y_train_tensor[-val_size:].to(device)\n",
    "        \n",
    "        val_outputs = model(val_inputs)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "        \n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        val_accuracy = accuracy_score(val_labels.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # æ—©åœé€»è¾‘\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        # å­¦ä¹ ç‡è°ƒæ•´\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# === ç»˜åˆ¶è®­ç»ƒå†å² ===\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('è®­ç»ƒæŸå¤±')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies)\n",
    "plt.title('éªŒè¯å‡†ç¡®ç‡')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# === LSTMå…ƒæ¨¡å‹è¯„ä¼° ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_probs_tensor = test_probs_tensor.to(device)\n",
    "    outputs = model(test_probs_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    predicted = predicted.cpu().numpy()\n",
    "    \n",
    "# è®¡ç®—æ€§èƒ½æŒ‡æ ‡\n",
    "lstm_acc = accuracy_score(y_test_int, predicted)\n",
    "lstm_report = classification_report(y_test_int, predicted)\n",
    "lstm_cm = confusion_matrix(y_test_int, predicted)\n",
    "\n",
    "print(\"\\n=== LSTMå…ƒæ¨¡å‹æ€§èƒ½ ===\")\n",
    "print(f\"âœ… LSTMå †å é›†æˆå‡†ç¡®ç‡: {lstm_acc:.4f}\")\n",
    "print(\"ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(lstm_report)\n",
    "\n",
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lstm_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "plt.xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "plt.ylabel('çœŸå®æ ‡ç­¾')\n",
    "plt.title('LSTMå…ƒæ¨¡å‹æ··æ·†çŸ©é˜µ')\n",
    "plt.savefig('lstm_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# === ä¸æŠ•ç¥¨ç³»ç»Ÿæ¯”è¾ƒ ===\n",
    "# å¯¹æµ‹è¯•é›†è¿›è¡Œå¤šæ•°æŠ•ç¥¨\n",
    "test_base_preds = [\n",
    "    np.argmax(xgb_test_preds, axis=1),\n",
    "    np.argmax(lgb_test_preds, axis=1),\n",
    "    np.argmax(rf_test_preds, axis=1)\n",
    "]\n",
    "\n",
    "# è½¬æ¢ä¸ºæ•°ç»„ä»¥ä¾¿æŠ•ç¥¨\n",
    "test_votes = np.array(test_base_preds).T\n",
    "vote_result = np.array([np.bincount(row).argmax() for row in test_votes])\n",
    "\n",
    "vote_acc = accuracy_score(y_test_int, vote_result)\n",
    "vote_report = classification_report(y_test_int, vote_result)\n",
    "\n",
    "print(\"\\n=== å¤šæ•°æŠ•ç¥¨ç³»ç»Ÿæ€§èƒ½ ===\")\n",
    "print(f\"âœ… æŠ•ç¥¨ç³»ç»Ÿå‡†ç¡®ç‡: {vote_acc:.4f}\")\n",
    "print(\"ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(vote_report)\n",
    "\n",
    "# åœ¨è¿™é‡ŒåŠ ä¸ŠåŸºçº¿æ¨¡å‹æŠ¥å‘Š\n",
    "print(\"\\n=== åŸºçº¿æ¨¡å‹åˆ†ç±»æŠ¥å‘Š ===\")\n",
    "for name, preds in zip(base_models, test_base_preds):\n",
    "    print(f\"\\n---- {name} ----\")\n",
    "    print(f\"å‡†ç¡®ç‡: {accuracy_score(y_test_int, preds):.4f}\")\n",
    "    print(\"åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(y_test_int, preds))\n",
    "\n",
    "# === æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹æ€§èƒ½ ===\n",
    "base_accs = [accuracy_score(y_test_int, preds) for preds in test_base_preds]\n",
    "\n",
    "model_names = base_models + ['æŠ•ç¥¨ç³»ç»Ÿ', 'LSTMå…ƒæ¨¡å‹']\n",
    "model_accs = base_accs + [vote_acc, lstm_acc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, model_accs, \n",
    "               color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "\n",
    "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ å‡†ç¡®ç‡æ•°å€¼\n",
    "for bar, acc_val in zip(bars, model_accs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc_val:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.ylim(0, max(model_accs) + 0.1)\n",
    "plt.xlabel('æ¨¡å‹')\n",
    "plt.ylabel('å‡†ç¡®ç‡')\n",
    "plt.title('å„ä¸ªæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æ¯”è¾ƒ')\n",
    "plt.savefig('lstm_model_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== å„ä¸ªæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æ¯”è¾ƒ ===\")\n",
    "for name, acc_val in zip(model_names, model_accs):\n",
    "    print(f\"{name}: {acc_val:.4f}\")\n",
    "\n",
    "# å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡åˆ†æ\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # è·å–ä¸€æ‰¹æµ‹è¯•æ•°æ®\n",
    "    sample_inputs = test_probs_tensor[:20].to(device)\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­å¹¶è·å–æ³¨æ„åŠ›æƒé‡\n",
    "    lstm_out, _ = model.lstm(sample_inputs)\n",
    "    attention_scores = model.attention(lstm_out)\n",
    "    attention_weights = torch.softmax(attention_scores, dim=1).cpu().numpy()\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡æ³¨æ„åŠ›æƒé‡\n",
    "    avg_attention = attention_weights.mean(axis=0).flatten()\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(base_models, avg_attention, color='skyblue')\n",
    "    plt.xlabel('åŸºç¡€æ¨¡å‹')\n",
    "    plt.ylabel('å¹³å‡æ³¨æ„åŠ›æƒé‡')\n",
    "    plt.title('LSTMå…ƒæ¨¡å‹å¯¹å„åŸºç¡€æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†é…')\n",
    "    plt.savefig('attention_weights.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n=== æ³¨æ„åŠ›æƒé‡åˆ†æ ===\")\n",
    "    for name, weight in zip(base_models, avg_attention):\n",
    "        print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nLSTMå †å é›†æˆè®­ç»ƒå®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
