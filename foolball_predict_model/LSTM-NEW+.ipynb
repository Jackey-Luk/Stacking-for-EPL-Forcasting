{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation of the TensorFlow/Keras LSTM model from `LSTM_New.ipynb`\n",
    "# -------------------------------------------------------------\n",
    "# Requirements: pandas, numpy, torch (>=2.0 recommended)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# Hyper‑parameters & paths\n",
    "# ---------------------------\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 34          # timesteps per sample\n",
    "INPUT_SIZE = 1        # one feature per timestep (after reshape)\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_CLASSES = 2       # Win / Non‑Win one‑hot originally → convert to class index 0/1\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "TRAIN_CSV = \"allAtt_onehot_large_train_new8.csv\"\n",
    "TEST_CSV  = \"allAtt_onehot_large_test_new8.csv\"\n",
    "MODEL_PATH = \"lstm_pytorch.pth\"\n",
    "\n",
    "# ---------------------------\n",
    "# Data loading & preprocessing\n",
    "# ---------------------------\n",
    "print(\"Loading data …\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# 34 特征列（与原 notebook 保持一致）\n",
    "X_train = train_df.iloc[:, 4:38].values.astype(np.float32).reshape(-1, SEQ_LEN, INPUT_SIZE)\n",
    "X_test  = test_df.iloc[:, 4:38].values.astype(np.float32).reshape(-1, SEQ_LEN, INPUT_SIZE)\n",
    "\n",
    "# one‑hot → 单一标签\n",
    "y_train = train_df.iloc[:, 38:].values.astype(np.float32).argmax(axis=1)\n",
    "y_test  = test_df.iloc[:, 38:].values.astype(np.float32).argmax(axis=1)\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "\n",
    "# ---------------------------\n",
    "# Model definition\n",
    "# ---------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.bn   = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc   = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        _, (h_n, _) = self.lstm(x)     # h_n: (1, batch, hidden)\n",
    "        h_n = h_n.squeeze(0)           # (batch, hidden)\n",
    "        h_n = self.bn(h_n)             # BatchNorm over features\n",
    "        logits = self.fc(h_n)\n",
    "        return logits\n",
    "\n",
    "# ---------------------------\n",
    "# Training setup\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = LSTMClassifier(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop\n",
    "# ---------------------------\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc  = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == y_batch).sum().item()\n",
    "            val_total   += y_batch.size(0)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}/{EPOCHS} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | val_acc {val_acc:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Save model\n",
    "# ---------------------------\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
